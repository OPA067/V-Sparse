<div align="left">

## ğŸ“£ Updates
* **[2025/01/18]**: We have released the complete training and testing code.
* **[2025/01/20]**: The paper has been submitted to the journal Neural Networks.

## âš¡ Framework

## ğŸ˜ Visualization

## ğŸš€ Quick Start

## ğŸ—ï¸ Acknowledgments

Q2: How are the recalibrated labels in Eq. (2) used in the training process?
A2: The recalibrated labels $\hat{l}_{ii} \in \{0, 1, 2\}$ generated by the SERD module (Eq. 2) 4 serve as reliability-based supervision weights that directly modulate the Cross-Modal Triplet (CMT) Loss:Mechanism: These labels are used to compute the positive matching probability $p_{ij}$ in Eq. (3)5.Clean Pairs ($\hat{l}=2$): Assigned the highest weight, encouraging the model to strictly align these high-confidence pairs.Noisy Pairs ($\hat{l}=0$): Assigned a weight of zero. This effectively "masks out" these pairs from the positive set during loss computation, preventing the model from overfitting to incorrect semantic associations.Uncertain Pairs ($\hat{l}=1$): Assigned an intermediate weight, allowing the model to learn from them without the strong penalty associated with confident positives.By plugging $p_{ij}$ into the CMT loss in Eq. (4)6666, the model dynamically adjusts its focus, prioritizing reliable data for optimization while mitigating the impact of noise.
